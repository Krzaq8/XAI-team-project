{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "import data_utils\n",
    "import pandas as pd\n",
    "import pytorch_tabr\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from typing import Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_columns', 70)\n",
    "pd.set_option('display.max_rows', 70)\n",
    "\n",
    "SEED = 42\n",
    "NUM_SPLITS = 10\n",
    "TARGET = \"decision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krzaq/xai/XAI-team-project/data_utils.py:31: DtypeWarning: Columns (4,11,12,16,17,18,19,20,40,41,42,43,44,45,52,53,54,55,56,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,108,110) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(\"SpeedDating.csv\", index_col=0)\n"
     ]
    }
   ],
   "source": [
    "reload(data_utils)\n",
    "from data_utils import get_dataset\n",
    "\n",
    "X, y = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>age_o</th>\n",
       "      <th>importance_same_race</th>\n",
       "      <th>importance_same_religion</th>\n",
       "      <th>pref_o_attractive</th>\n",
       "      <th>pref_o_sincere</th>\n",
       "      <th>pref_o_intelligence</th>\n",
       "      <th>pref_o_funny</th>\n",
       "      <th>pref_o_ambitious</th>\n",
       "      <th>pref_o_shared_interests</th>\n",
       "      <th>attractive_o</th>\n",
       "      <th>sinsere_o</th>\n",
       "      <th>intelligence_o</th>\n",
       "      <th>funny_o</th>\n",
       "      <th>ambitous_o</th>\n",
       "      <th>shared_interests_o</th>\n",
       "      <th>attractive_important</th>\n",
       "      <th>sincere_important</th>\n",
       "      <th>intellicence_important</th>\n",
       "      <th>funny_important</th>\n",
       "      <th>ambtition_important</th>\n",
       "      <th>shared_interests_important</th>\n",
       "      <th>attractive</th>\n",
       "      <th>sincere</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>funny</th>\n",
       "      <th>ambition</th>\n",
       "      <th>attractive_partner</th>\n",
       "      <th>sincere_partner</th>\n",
       "      <th>intelligence_partner</th>\n",
       "      <th>funny_partner</th>\n",
       "      <th>ambition_partner</th>\n",
       "      <th>shared_interests_partner</th>\n",
       "      <th>sports</th>\n",
       "      <th>tvsports</th>\n",
       "      <th>exercise</th>\n",
       "      <th>dining</th>\n",
       "      <th>museums</th>\n",
       "      <th>art</th>\n",
       "      <th>hiking</th>\n",
       "      <th>gaming</th>\n",
       "      <th>clubbing</th>\n",
       "      <th>reading</th>\n",
       "      <th>tv</th>\n",
       "      <th>theater</th>\n",
       "      <th>movies</th>\n",
       "      <th>concerts</th>\n",
       "      <th>music</th>\n",
       "      <th>shopping</th>\n",
       "      <th>yoga</th>\n",
       "      <th>interests_correlate</th>\n",
       "      <th>expected_happy_with_sd_people</th>\n",
       "      <th>expected_num_matches</th>\n",
       "      <th>expected_num_interested_in_me</th>\n",
       "      <th>like</th>\n",
       "      <th>guess_prob_liked</th>\n",
       "      <th>race_?</th>\n",
       "      <th>race_Asian/Pacific Islander/Asian-American</th>\n",
       "      <th>race_Black/African American</th>\n",
       "      <th>race_European/Caucasian-American</th>\n",
       "      <th>race_Latino/Hispanic American</th>\n",
       "      <th>race_Other</th>\n",
       "      <th>race_o_?</th>\n",
       "      <th>race_o_Asian/Pacific Islander/Asian-American</th>\n",
       "      <th>race_o_Black/African American</th>\n",
       "      <th>race_o_European/Caucasian-American</th>\n",
       "      <th>race_o_Latino/Hispanic American</th>\n",
       "      <th>race_o_Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "      <td>8378.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.499403</td>\n",
       "      <td>26.358928</td>\n",
       "      <td>26.364999</td>\n",
       "      <td>3.784793</td>\n",
       "      <td>3.651645</td>\n",
       "      <td>22.495347</td>\n",
       "      <td>17.396867</td>\n",
       "      <td>20.270759</td>\n",
       "      <td>17.459714</td>\n",
       "      <td>10.685375</td>\n",
       "      <td>11.845930</td>\n",
       "      <td>6.190411</td>\n",
       "      <td>7.175256</td>\n",
       "      <td>7.369301</td>\n",
       "      <td>6.400599</td>\n",
       "      <td>6.778409</td>\n",
       "      <td>5.474870</td>\n",
       "      <td>22.514632</td>\n",
       "      <td>17.396389</td>\n",
       "      <td>20.265613</td>\n",
       "      <td>17.457043</td>\n",
       "      <td>10.682539</td>\n",
       "      <td>11.845111</td>\n",
       "      <td>7.084733</td>\n",
       "      <td>8.294935</td>\n",
       "      <td>7.704460</td>\n",
       "      <td>8.403965</td>\n",
       "      <td>7.578388</td>\n",
       "      <td>6.189995</td>\n",
       "      <td>7.175164</td>\n",
       "      <td>7.368597</td>\n",
       "      <td>6.400598</td>\n",
       "      <td>6.777524</td>\n",
       "      <td>5.474559</td>\n",
       "      <td>6.425232</td>\n",
       "      <td>4.575491</td>\n",
       "      <td>6.245813</td>\n",
       "      <td>7.783829</td>\n",
       "      <td>6.985781</td>\n",
       "      <td>6.714544</td>\n",
       "      <td>5.737077</td>\n",
       "      <td>3.881191</td>\n",
       "      <td>5.745993</td>\n",
       "      <td>7.678515</td>\n",
       "      <td>5.304133</td>\n",
       "      <td>6.776118</td>\n",
       "      <td>7.919629</td>\n",
       "      <td>6.825401</td>\n",
       "      <td>7.851066</td>\n",
       "      <td>5.631281</td>\n",
       "      <td>4.339197</td>\n",
       "      <td>0.196010</td>\n",
       "      <td>5.534131</td>\n",
       "      <td>3.207814</td>\n",
       "      <td>5.570556</td>\n",
       "      <td>6.134087</td>\n",
       "      <td>5.207523</td>\n",
       "      <td>0.007520</td>\n",
       "      <td>0.236572</td>\n",
       "      <td>0.050131</td>\n",
       "      <td>0.564216</td>\n",
       "      <td>0.079255</td>\n",
       "      <td>0.062306</td>\n",
       "      <td>0.008713</td>\n",
       "      <td>0.236095</td>\n",
       "      <td>0.050131</td>\n",
       "      <td>0.563619</td>\n",
       "      <td>0.079255</td>\n",
       "      <td>0.062187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500029</td>\n",
       "      <td>3.546480</td>\n",
       "      <td>3.541458</td>\n",
       "      <td>2.832258</td>\n",
       "      <td>2.791978</td>\n",
       "      <td>12.502851</td>\n",
       "      <td>7.006484</td>\n",
       "      <td>6.746767</td>\n",
       "      <td>6.049825</td>\n",
       "      <td>6.087291</td>\n",
       "      <td>6.313565</td>\n",
       "      <td>1.925469</td>\n",
       "      <td>1.710499</td>\n",
       "      <td>1.521919</td>\n",
       "      <td>1.911629</td>\n",
       "      <td>1.715024</td>\n",
       "      <td>2.012929</td>\n",
       "      <td>12.528179</td>\n",
       "      <td>7.013394</td>\n",
       "      <td>6.750943</td>\n",
       "      <td>6.052827</td>\n",
       "      <td>6.088588</td>\n",
       "      <td>6.316038</td>\n",
       "      <td>1.387008</td>\n",
       "      <td>1.398611</td>\n",
       "      <td>1.554486</td>\n",
       "      <td>1.069839</td>\n",
       "      <td>1.767135</td>\n",
       "      <td>1.926513</td>\n",
       "      <td>1.711300</td>\n",
       "      <td>1.522814</td>\n",
       "      <td>1.912453</td>\n",
       "      <td>1.716119</td>\n",
       "      <td>2.014356</td>\n",
       "      <td>2.606645</td>\n",
       "      <td>2.788632</td>\n",
       "      <td>2.407425</td>\n",
       "      <td>1.746573</td>\n",
       "      <td>2.042532</td>\n",
       "      <td>2.252709</td>\n",
       "      <td>2.558059</td>\n",
       "      <td>2.608122</td>\n",
       "      <td>2.490391</td>\n",
       "      <td>1.997082</td>\n",
       "      <td>2.517181</td>\n",
       "      <td>2.224588</td>\n",
       "      <td>1.692888</td>\n",
       "      <td>2.146091</td>\n",
       "      <td>1.783358</td>\n",
       "      <td>2.596582</td>\n",
       "      <td>2.704767</td>\n",
       "      <td>0.300663</td>\n",
       "      <td>1.723574</td>\n",
       "      <td>2.267192</td>\n",
       "      <td>2.207052</td>\n",
       "      <td>1.814717</td>\n",
       "      <td>2.089919</td>\n",
       "      <td>0.086395</td>\n",
       "      <td>0.425003</td>\n",
       "      <td>0.218229</td>\n",
       "      <td>0.495889</td>\n",
       "      <td>0.270153</td>\n",
       "      <td>0.241725</td>\n",
       "      <td>0.092943</td>\n",
       "      <td>0.424706</td>\n",
       "      <td>0.218229</td>\n",
       "      <td>0.495966</td>\n",
       "      <td>0.270153</td>\n",
       "      <td>0.241509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.830000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>17.650000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.570556</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.369301</td>\n",
       "      <td>6.400599</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.474870</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.147468</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.368597</td>\n",
       "      <td>6.400598</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.474559</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.245813</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.570556</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>23.260000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>23.260000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.690000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.570556</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gender          age        age_o  importance_same_race  \\\n",
       "count  8378.000000  8378.000000  8378.000000           8378.000000   \n",
       "mean      0.499403    26.358928    26.364999              3.784793   \n",
       "std       0.500029     3.546480     3.541458              2.832258   \n",
       "min       0.000000    18.000000    18.000000              0.000000   \n",
       "25%       0.000000    24.000000    24.000000              1.000000   \n",
       "50%       0.000000    26.000000    26.000000              3.000000   \n",
       "75%       1.000000    28.000000    28.000000              6.000000   \n",
       "max       1.000000    55.000000    55.000000             10.000000   \n",
       "\n",
       "       importance_same_religion  pref_o_attractive  pref_o_sincere  \\\n",
       "count               8378.000000        8378.000000     8378.000000   \n",
       "mean                   3.651645          22.495347       17.396867   \n",
       "std                    2.791978          12.502851        7.006484   \n",
       "min                    1.000000           0.000000        0.000000   \n",
       "25%                    1.000000          15.000000       15.000000   \n",
       "50%                    3.000000          20.000000       18.000000   \n",
       "75%                    6.000000          25.000000       20.000000   \n",
       "max                   10.000000         100.000000       60.000000   \n",
       "\n",
       "       pref_o_intelligence  pref_o_funny  pref_o_ambitious  \\\n",
       "count          8378.000000   8378.000000       8378.000000   \n",
       "mean             20.270759     17.459714         10.685375   \n",
       "std               6.746767      6.049825          6.087291   \n",
       "min               0.000000      0.000000          0.000000   \n",
       "25%              17.650000     15.000000          5.000000   \n",
       "50%              20.000000     18.000000         10.000000   \n",
       "75%              23.260000     20.000000         15.000000   \n",
       "max              50.000000     50.000000         53.000000   \n",
       "\n",
       "       pref_o_shared_interests  attractive_o    sinsere_o  intelligence_o  \\\n",
       "count              8378.000000   8378.000000  8378.000000     8378.000000   \n",
       "mean                 11.845930      6.190411     7.175256        7.369301   \n",
       "std                   6.313565      1.925469     1.710499        1.521919   \n",
       "min                   0.000000      0.000000     0.000000        0.000000   \n",
       "25%                  10.000000      5.000000     6.000000        7.000000   \n",
       "50%                  11.360000      6.000000     7.000000        7.369301   \n",
       "75%                  15.690000      8.000000     8.000000        8.000000   \n",
       "max                  30.000000     10.500000    10.000000       10.000000   \n",
       "\n",
       "           funny_o   ambitous_o  shared_interests_o  attractive_important  \\\n",
       "count  8378.000000  8378.000000         8378.000000           8378.000000   \n",
       "mean      6.400599     6.778409            5.474870             22.514632   \n",
       "std       1.911629     1.715024            2.012929             12.528179   \n",
       "min       0.000000     0.000000            0.000000              0.000000   \n",
       "25%       5.000000     6.000000            4.000000             15.000000   \n",
       "50%       6.400599     7.000000            5.474870             20.000000   \n",
       "75%       8.000000     8.000000            7.000000             25.000000   \n",
       "max      11.000000    10.000000           10.000000            100.000000   \n",
       "\n",
       "       sincere_important  intellicence_important  funny_important  \\\n",
       "count        8378.000000             8378.000000      8378.000000   \n",
       "mean           17.396389               20.265613        17.457043   \n",
       "std             7.013394                6.750943         6.052827   \n",
       "min             0.000000                0.000000         0.000000   \n",
       "25%            15.000000               17.650000        15.000000   \n",
       "50%            18.180000               20.000000        18.000000   \n",
       "75%            20.000000               23.260000        20.000000   \n",
       "max            60.000000               50.000000        50.000000   \n",
       "\n",
       "       ambtition_important  shared_interests_important   attractive  \\\n",
       "count          8378.000000                 8378.000000  8378.000000   \n",
       "mean             10.682539                   11.845111     7.084733   \n",
       "std               6.088588                    6.316038     1.387008   \n",
       "min               0.000000                    0.000000     2.000000   \n",
       "25%               5.000000                   10.000000     6.000000   \n",
       "50%              10.000000                   11.110000     7.000000   \n",
       "75%              15.000000                   15.690000     8.000000   \n",
       "max              53.000000                   30.000000    10.000000   \n",
       "\n",
       "           sincere  intelligence        funny     ambition  \\\n",
       "count  8378.000000   8378.000000  8378.000000  8378.000000   \n",
       "mean      8.294935      7.704460     8.403965     7.578388   \n",
       "std       1.398611      1.554486     1.069839     1.767135   \n",
       "min       2.000000      2.000000     3.000000     2.000000   \n",
       "25%       8.000000      7.000000     8.000000     7.000000   \n",
       "50%       8.147468      8.000000     8.000000     8.000000   \n",
       "75%       9.000000      9.000000     9.000000     9.000000   \n",
       "max      10.000000     10.000000    10.000000    10.000000   \n",
       "\n",
       "       attractive_partner  sincere_partner  intelligence_partner  \\\n",
       "count         8378.000000      8378.000000           8378.000000   \n",
       "mean             6.189995         7.175164              7.368597   \n",
       "std              1.926513         1.711300              1.522814   \n",
       "min              0.000000         0.000000              0.000000   \n",
       "25%              5.000000         6.000000              7.000000   \n",
       "50%              6.000000         7.000000              7.368597   \n",
       "75%              8.000000         8.000000              8.000000   \n",
       "max             10.000000        10.000000             10.000000   \n",
       "\n",
       "       funny_partner  ambition_partner  shared_interests_partner       sports  \\\n",
       "count    8378.000000       8378.000000               8378.000000  8378.000000   \n",
       "mean        6.400598          6.777524                  5.474559     6.425232   \n",
       "std         1.912453          1.716119                  2.014356     2.606645   \n",
       "min         0.000000          0.000000                  0.000000     1.000000   \n",
       "25%         5.000000          6.000000                  4.000000     5.000000   \n",
       "50%         6.400598          7.000000                  5.474559     7.000000   \n",
       "75%         8.000000          8.000000                  7.000000     9.000000   \n",
       "max        10.000000         10.000000                 10.000000    10.000000   \n",
       "\n",
       "          tvsports     exercise       dining      museums          art  \\\n",
       "count  8378.000000  8378.000000  8378.000000  8378.000000  8378.000000   \n",
       "mean      4.575491     6.245813     7.783829     6.985781     6.714544   \n",
       "std       2.788632     2.407425     1.746573     2.042532     2.252709   \n",
       "min       1.000000     1.000000     1.000000     0.000000     0.000000   \n",
       "25%       2.000000     5.000000     7.000000     6.000000     5.000000   \n",
       "50%       4.000000     6.245813     8.000000     7.000000     7.000000   \n",
       "75%       7.000000     8.000000     9.000000     8.000000     8.000000   \n",
       "max      10.000000    10.000000    10.000000    10.000000    10.000000   \n",
       "\n",
       "            hiking       gaming     clubbing      reading           tv  \\\n",
       "count  8378.000000  8378.000000  8378.000000  8378.000000  8378.000000   \n",
       "mean      5.737077     3.881191     5.745993     7.678515     5.304133   \n",
       "std       2.558059     2.608122     2.490391     1.997082     2.517181   \n",
       "min       0.000000     0.000000     0.000000     1.000000     1.000000   \n",
       "25%       4.000000     2.000000     4.000000     7.000000     3.000000   \n",
       "50%       6.000000     3.000000     6.000000     8.000000     6.000000   \n",
       "75%       8.000000     6.000000     8.000000     9.000000     7.000000   \n",
       "max      10.000000    14.000000    10.000000    13.000000    10.000000   \n",
       "\n",
       "           theater       movies     concerts        music     shopping  \\\n",
       "count  8378.000000  8378.000000  8378.000000  8378.000000  8378.000000   \n",
       "mean      6.776118     7.919629     6.825401     7.851066     5.631281   \n",
       "std       2.224588     1.692888     2.146091     1.783358     2.596582   \n",
       "min       0.000000     0.000000     0.000000     1.000000     1.000000   \n",
       "25%       5.000000     7.000000     5.000000     7.000000     4.000000   \n",
       "50%       7.000000     8.000000     7.000000     8.000000     6.000000   \n",
       "75%       8.000000     9.000000     8.000000     9.000000     8.000000   \n",
       "max      10.000000    10.000000    10.000000    10.000000    10.000000   \n",
       "\n",
       "              yoga  interests_correlate  expected_happy_with_sd_people  \\\n",
       "count  8378.000000          8378.000000                    8378.000000   \n",
       "mean      4.339197             0.196010                       5.534131   \n",
       "std       2.704767             0.300663                       1.723574   \n",
       "min       0.000000            -0.830000                       1.000000   \n",
       "25%       2.000000            -0.010000                       5.000000   \n",
       "50%       4.000000             0.200000                       6.000000   \n",
       "75%       6.000000             0.430000                       7.000000   \n",
       "max      10.000000             0.910000                      10.000000   \n",
       "\n",
       "       expected_num_matches  expected_num_interested_in_me         like  \\\n",
       "count           8378.000000                    8378.000000  8378.000000   \n",
       "mean               3.207814                       5.570556     6.134087   \n",
       "std                2.267192                       2.207052     1.814717   \n",
       "min                0.000000                       0.000000     0.000000   \n",
       "25%                2.000000                       5.570556     5.000000   \n",
       "50%                3.000000                       5.570556     6.000000   \n",
       "75%                4.000000                       5.570556     7.000000   \n",
       "max               18.000000                      20.000000    10.000000   \n",
       "\n",
       "       guess_prob_liked       race_?  \\\n",
       "count       8378.000000  8378.000000   \n",
       "mean           5.207523     0.007520   \n",
       "std            2.089919     0.086395   \n",
       "min            0.000000     0.000000   \n",
       "25%            4.000000     0.000000   \n",
       "50%            5.000000     0.000000   \n",
       "75%            7.000000     0.000000   \n",
       "max           10.000000     1.000000   \n",
       "\n",
       "       race_Asian/Pacific Islander/Asian-American  \\\n",
       "count                                 8378.000000   \n",
       "mean                                     0.236572   \n",
       "std                                      0.425003   \n",
       "min                                      0.000000   \n",
       "25%                                      0.000000   \n",
       "50%                                      0.000000   \n",
       "75%                                      0.000000   \n",
       "max                                      1.000000   \n",
       "\n",
       "       race_Black/African American  race_European/Caucasian-American  \\\n",
       "count                  8378.000000                       8378.000000   \n",
       "mean                      0.050131                          0.564216   \n",
       "std                       0.218229                          0.495889   \n",
       "min                       0.000000                          0.000000   \n",
       "25%                       0.000000                          0.000000   \n",
       "50%                       0.000000                          1.000000   \n",
       "75%                       0.000000                          1.000000   \n",
       "max                       1.000000                          1.000000   \n",
       "\n",
       "       race_Latino/Hispanic American   race_Other     race_o_?  \\\n",
       "count                    8378.000000  8378.000000  8378.000000   \n",
       "mean                        0.079255     0.062306     0.008713   \n",
       "std                         0.270153     0.241725     0.092943   \n",
       "min                         0.000000     0.000000     0.000000   \n",
       "25%                         0.000000     0.000000     0.000000   \n",
       "50%                         0.000000     0.000000     0.000000   \n",
       "75%                         0.000000     0.000000     0.000000   \n",
       "max                         1.000000     1.000000     1.000000   \n",
       "\n",
       "       race_o_Asian/Pacific Islander/Asian-American  \\\n",
       "count                                   8378.000000   \n",
       "mean                                       0.236095   \n",
       "std                                        0.424706   \n",
       "min                                        0.000000   \n",
       "25%                                        0.000000   \n",
       "50%                                        0.000000   \n",
       "75%                                        0.000000   \n",
       "max                                        1.000000   \n",
       "\n",
       "       race_o_Black/African American  race_o_European/Caucasian-American  \\\n",
       "count                    8378.000000                         8378.000000   \n",
       "mean                        0.050131                            0.563619   \n",
       "std                         0.218229                            0.495966   \n",
       "min                         0.000000                            0.000000   \n",
       "25%                         0.000000                            0.000000   \n",
       "50%                         0.000000                            1.000000   \n",
       "75%                         0.000000                            1.000000   \n",
       "max                         1.000000                            1.000000   \n",
       "\n",
       "       race_o_Latino/Hispanic American  race_o_Other  \n",
       "count                      8378.000000   8378.000000  \n",
       "mean                          0.079255      0.062187  \n",
       "std                           0.270153      0.241509  \n",
       "min                           0.000000      0.000000  \n",
       "25%                           0.000000      0.000000  \n",
       "50%                           0.000000      0.000000  \n",
       "75%                           0.000000      0.000000  \n",
       "max                           1.000000      1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krzaq/xai/XAI-team-project/pytorch_tabr/base_model.py:258: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d2522bd770422dbc1f98251ac3acb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " epochs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104fc0061f96437588b20e4333386c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " batches:   0%|          | 0/754 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krzaq/.local/share/mamba/envs/tabr/lib/python3.9/site-packages/faiss/contrib/torch_utils.py:51: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 4)\n",
      "/home/krzaq/.local/share/mamba/envs/tabr/lib/python3.9/site-packages/faiss/contrib/torch_utils.py:65: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  x.storage().data_ptr() + x.storage_offset() * 8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e3883f3ad6438a9694989f371536a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       " batches:   0%|          | 0/754 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/share/mamba/envs/tabr/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39miloc[test_idx, :], y\u001b[38;5;241m.\u001b[39miloc[test_idx]\n\u001b[1;32m     21\u001b[0m tabr_classifier\u001b[38;5;241m.\u001b[39mfit(X_train\u001b[38;5;241m.\u001b[39mvalues, y_train\u001b[38;5;241m.\u001b[39mvalues, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSplit accuracy: \u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mtabr_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_test)))\n\u001b[1;32m     23\u001b[0m s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(tabr_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_test))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy for class 1 [person wanted to match]\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39msum( np\u001b[38;5;241m.\u001b[39mlogical_and(tabr_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, tabr_classifier\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;241m==\u001b[39m y_test))\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(y_test))\n",
      "File \u001b[0;32m~/xai/XAI-team-project/pytorch_tabr/base_model.py:567\u001b[0m, in \u001b[0;36mTabRBase.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    554\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    Make predictions on a batch (valid)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03m        Predictions of the regression problem\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_func(preds)\n",
      "File \u001b[0;32m~/xai/XAI-team-project/pytorch_tabr/models.py:158\u001b[0m, in \u001b[0;36mTabRClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    151\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    152\u001b[0m         PredictDataset(X),\n\u001b[1;32m    153\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m    154\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    157\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_nb, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m    159\u001b[0m     data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(data)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    161\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(\n\u001b[1;32m    162\u001b[0m         x\u001b[38;5;241m=\u001b[39mdata,\n\u001b[1;32m    163\u001b[0m         y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    166\u001b[0m         context_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_size,\n\u001b[1;32m    167\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/tabr/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/tabr/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/tabr/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/tabr/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/xai/XAI-team-project/pytorch_tabr/dataloader.py:78\u001b[0m, in \u001b[0;36mPredictDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 78\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/tabr/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/share/mamba/envs/tabr/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "reload(pytorch_tabr)\n",
    "from pytorch_tabr import TabRClassifier\n",
    "# Train and test the model\n",
    "kf = StratifiedKFold(n_splits=NUM_SPLITS, shuffle=True, random_state=SEED)\n",
    "\n",
    "tabr_classifier = TabRClassifier(\n",
    "    type_embeddings=\"one-hot\",\n",
    "    device_name=\"cuda\",\n",
    "    optimizer_params={\"lr\": 2e-4},\n",
    "    d_main=96,\n",
    "    context_size=96,\n",
    "    # selection_function_name=\"sparsemax\",\n",
    "    # context_dropout=0.5,\n",
    "    # context_sample_size=2000,\n",
    "    # num_embeddings={\"type\": \"PLREmbeddings\", \"n_frequencies\": 32, \"frequency_scale\": 32, \"d_embedding\": 32, \"lite\": False},\n",
    ")\n",
    "s=0\n",
    "for train_idx, test_idx in kf.split(X, y):\n",
    "    X_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]\n",
    "    X_test, y_test = X.iloc[test_idx, :], y.iloc[test_idx]\n",
    "    tabr_classifier.fit(X_train.values, y_train.values, max_epochs=2, batch_size=10)\n",
    "    print('Split accuracy: ', np.mean(tabr_classifier.predict(X_test).values == np.array(y_test)))\n",
    "    s += np.mean(tabr_classifier.predict(X_test) == np.array(y_test))\n",
    "    print('Accuracy for class 1 [person wanted to match]', np.sum( np.logical_and(tabr_classifier.predict(X_test) == 1, tabr_classifier.predict(X_test) == y_test))/np.sum(y_test))\n",
    "print('Average accuracy:')\n",
    "print(s/NUM_SPLITS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
